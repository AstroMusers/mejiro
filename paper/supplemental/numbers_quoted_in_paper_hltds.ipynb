{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY = 'hltds_deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from hydra import initialize, compose\n",
    "import pickle\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# set paths to various directories based on the machine this code is being executed on\n",
    "try:\n",
    "    with initialize(version_base=None, config_path='config'):  \n",
    "        config = compose(config_name=f'config_{SURVEY}.yaml')  # overrides=['machine=uzay']\n",
    "except:\n",
    "    with initialize(version_base=None, config_path='../../config'):  \n",
    "        config = compose(config_name=f'config_{SURVEY}.yaml')  # overrides=['machine=uzay']\n",
    "\n",
    "array_dir, data_dir, figure_dir, pickle_dir, repo_dir  = config.machine.array_dir, config.machine.data_dir, config.machine.figure_dir, config.machine.pickle_dir, config.machine.repo_dir\n",
    "\n",
    "# enable use of local modules\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)\n",
    "\n",
    "# set matplotlib style\n",
    "plt.style.use(f'{repo_dir}/mejiro/mplstyle/science.mplstyle')\n",
    "\n",
    "from mejiro.utils import util\n",
    "\n",
    "survey_params = util.hydra_to_dict(config.survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = util.hydra_to_dict(config.pipeline)\n",
    "debugging = pipeline_params['debugging']\n",
    "\n",
    "if debugging:\n",
    "    pipeline_dir = f'{config.machine.pipeline_dir}_{SURVEY}'\n",
    "else:\n",
    "    pipeline_dir = config.machine.pipeline_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectable and candidate lens counts\n",
    "\n",
    "Number of detectable strong lenses per square degree of simulated area, candidate strong lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m detectable_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetectable_pop.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m util\u001b[38;5;241m.\u001b[39mdelete_if_exists(detectable_csv)\n\u001b[0;32m----> 6\u001b[0m detectable_df \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_all_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetectable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetectable_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m survey_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_population\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     total_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_pop.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/mejiro/mejiro/utils/util.py:461\u001b[0m, in \u001b[0;36mcombine_all_csvs\u001b[0;34m(path, prefix, filename)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# concatenate CSVs\u001b[39;00m\n\u001b[1;32m    460\u001b[0m pd_list \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m csv_files]\n\u001b[0;32m--> 461\u001b[0m df_res \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO fix FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# save as combined CSV\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/bwedig/.conda/envs/mejiro/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/bwedig/.conda/envs/mejiro/lib/python3.10/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/data/bwedig/.conda/envs/mejiro/lib/python3.10/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# combine CSVs from each run into one CSV\n",
    "data_dir = os.path.join(pipeline_dir, '00')\n",
    "\n",
    "detectable_csv = os.path.join(data_dir, 'detectable_pop.csv')\n",
    "util.delete_if_exists(detectable_csv)\n",
    "detectable_df = util.combine_all_csvs(data_dir, 'detectable', detectable_csv)\n",
    "\n",
    "if survey_params['total_population']:\n",
    "    total_csv = os.path.join(data_dir, 'total_pop.csv')\n",
    "    util.delete_if_exists(total_csv)\n",
    "    total_df = util.combine_all_csvs(data_dir, 'total', total_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540 run(s) of 0.5 sq. deg. each gives 270.00 sq. deg. total\n"
     ]
    }
   ],
   "source": [
    "survey_area = survey_params['area']\n",
    "runs = survey_params['runs']\n",
    "\n",
    "total_area = survey_area * runs\n",
    "print(f'{runs} run(s) of {survey_area} sq. deg. each gives {total_area:.2f} sq. deg. total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14388 detectable strong lenses\n",
      "3086132 total candidate strong lenses\n",
      "Fraction of candidate strong lenses that are detectable: 0.00466\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(detectable_df)} detectable strong lenses')\n",
    "\n",
    "if survey_params['total_population']:\n",
    "    print(f'{len(total_df)} total candidate strong lenses')\n",
    "    fraction_detectable = len(detectable_df) / len(total_df)\n",
    "    print(f'Fraction of candidate strong lenses that are detectable: {fraction_detectable:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectable strong lenses per sq. deg.: 53.29\n",
      "Candidate strong lenses per sq. deg.: 11430\n"
     ]
    }
   ],
   "source": [
    "det_per_sq_deg = len(detectable_df) / total_area\n",
    "print(f'Detectable strong lenses per sq. deg.: {det_per_sq_deg:.2f}')\n",
    "\n",
    "if survey_params['total_population']:\n",
    "    total_per_sq_deg = len(total_df) / total_area\n",
    "    print(f'Candidate strong lenses per sq. deg.: {round(total_per_sq_deg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectable strong lenses per exposure: 14.97\n"
     ]
    }
   ],
   "source": [
    "det_per_exposure = det_per_sq_deg * 0.281\n",
    "print(f'Detectable strong lenses per exposure: {det_per_exposure:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 characterizable strong lenses (SNR > 200)\n",
      "0.24 characterizable per sq. deg.\n",
      "Characterizable strong lenses per exposure: 0.07\n"
     ]
    }
   ],
   "source": [
    "snr_threshold = 200\n",
    "high_snr_df = detectable_df[detectable_df['snr'] > snr_threshold]\n",
    "high_snr_det_per_sq_deg = len(high_snr_df) / total_area\n",
    "high_snr_det_per_exposure = high_snr_det_per_sq_deg * 0.281\n",
    "\n",
    "print(f'{len(high_snr_df)} characterizable strong lenses (SNR > {snr_threshold})')\n",
    "print(f'{high_snr_det_per_sq_deg:.2f} characterizable per sq. deg.')\n",
    "print(f'Characterizable strong lenses per exposure: {high_snr_det_per_exposure:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90591 detectable strong lenses in HLWAS\n",
      "409 characterizable strong lenses in HLWAS\n"
     ]
    }
   ],
   "source": [
    "# HLWAS\n",
    "print(f'{round(det_per_sq_deg * 1700)} detectable strong lenses in HLWAS')\n",
    "print(f'{round(high_snr_det_per_sq_deg * 1700)} characterizable strong lenses in HLWAS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mejiro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
