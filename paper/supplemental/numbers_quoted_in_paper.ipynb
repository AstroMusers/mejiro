{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from hydra import initialize, compose\n",
    "import pickle\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# set paths to various directories based on the machine this code is being executed on\n",
    "try:\n",
    "    with initialize(version_base=None, config_path='config'):  \n",
    "        config = compose(config_name='config.yaml')  # overrides=['machine=uzay']\n",
    "except:\n",
    "    with initialize(version_base=None, config_path='../../config'):  \n",
    "        config = compose(config_name='config.yaml')  # overrides=['machine=uzay']\n",
    "\n",
    "array_dir, data_dir, figure_dir, pickle_dir, repo_dir  = config.machine.array_dir, config.machine.data_dir, config.machine.figure_dir, config.machine.pickle_dir, config.machine.repo_dir\n",
    "\n",
    "# enable use of local modules\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)\n",
    "\n",
    "# set matplotlib style\n",
    "plt.style.use(f'{repo_dir}/mejiro/mplstyle/science.mplstyle')\n",
    "\n",
    "from mejiro.utils import util\n",
    "\n",
    "survey_params = util.hydra_to_dict(config.survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = util.hydra_to_dict(config.pipeline)\n",
    "debugging = pipeline_params['debugging']\n",
    "\n",
    "if debugging:\n",
    "    pipeline_dir = f'{config.machine.pipeline_dir}_dev'\n",
    "else:\n",
    "    pipeline_dir = config.machine.pipeline_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectable and candidate lens counts\n",
    "\n",
    "Number of detectable strong lenses per square degree of simulated area, candidate strong lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote combined CSV to /data/bwedig/mejiro/pipeline_dev/00/detectable_pop.csv\n",
      "Wrote combined CSV to /data/bwedig/mejiro/pipeline_dev/00/total_pop.csv\n"
     ]
    }
   ],
   "source": [
    "# combine CSVs from each run into one CSV\n",
    "data_dir = os.path.join(pipeline_dir, '00')\n",
    "\n",
    "detectable_csv = os.path.join(data_dir, 'detectable_pop.csv')\n",
    "util.delete_if_exists(detectable_csv)\n",
    "detectable_df = util.combine_all_csvs(data_dir, 'detectable', detectable_csv)\n",
    "\n",
    "if survey_params['total_population']:\n",
    "    total_csv = os.path.join(data_dir, 'total_pop.csv')\n",
    "    util.delete_if_exists(total_csv)\n",
    "    total_df = util.combine_all_csvs(data_dir, 'total', total_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 run(s) of 0.1 sq. deg. each gives 1.80 sq. deg. total\n"
     ]
    }
   ],
   "source": [
    "survey_area = survey_params['area']\n",
    "runs = survey_params['runs']\n",
    "\n",
    "total_area = survey_area * runs\n",
    "print(f'{runs} run(s) of {survey_area} sq. deg. each gives {total_area:.2f} sq. deg. total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 detectable strong lenses\n",
      "14138 total candidate strong lenses\n",
      "Fraction of candidate strong lenses that are detectable: 0.00729\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(detectable_df)} detectable strong lenses')\n",
    "\n",
    "if survey_params['total_population']:\n",
    "    print(f'{len(total_df)} total candidate strong lenses')\n",
    "    fraction_detectable = len(detectable_df) / len(total_df)\n",
    "    print(f'Fraction of candidate strong lenses that are detectable: {fraction_detectable:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectable strong lenses per sq. deg.: 57.22\n",
      "Candidate strong lenses per sq. deg.: 7854\n"
     ]
    }
   ],
   "source": [
    "det_per_sq_deg = len(detectable_df) / total_area\n",
    "print(f'Detectable strong lenses per sq. deg.: {det_per_sq_deg:.2f}')\n",
    "\n",
    "if survey_params['total_population']:\n",
    "    total_per_sq_deg = len(total_df) / total_area\n",
    "    print(f'Candidate strong lenses per sq. deg.: {round(total_per_sq_deg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectable strong lenses per exposure: 16.08\n"
     ]
    }
   ],
   "source": [
    "det_per_exposure = det_per_sq_deg * 0.281\n",
    "print(f'Detectable strong lenses per exposure: {det_per_exposure:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 characterizable strong lenses (SNR > 50)\n",
      "20.00 characterizable per sq. deg.\n",
      "Characterizable strong lenses per exposure: 5.62\n"
     ]
    }
   ],
   "source": [
    "snr_threshold = 50\n",
    "high_snr_df = detectable_df[detectable_df['snr'] > snr_threshold]\n",
    "high_snr_det_per_sq_deg = len(high_snr_df) / total_area\n",
    "high_snr_det_per_exposure = high_snr_det_per_sq_deg * 0.281\n",
    "\n",
    "print(f'{len(high_snr_df)} characterizable strong lenses (SNR > {snr_threshold})')\n",
    "print(f'{high_snr_det_per_sq_deg:.2f} characterizable per sq. deg.')\n",
    "print(f'Characterizable strong lenses per exposure: {high_snr_det_per_exposure:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278 detectable strong lenses in HLWAS\n",
      "34000 characterizable strong lenses in HLWAS\n"
     ]
    }
   ],
   "source": [
    "# HLWAS\n",
    "print(f'{round(det_per_sq_deg * 1700)} detectable strong lenses in HLWAS')\n",
    "print(f'{round(high_snr_det_per_sq_deg * 1700)} characterizable strong lenses in HLWAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much work are the various filters doing?\n",
    "\n",
    "`filter_1` is Einstein radius and half-light radius, and `filter_2` is SNR. The other detectability criteria are combined into the difference between candidate and detectable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5958 (42.14%) candidate strong lenses caught in filter 1\n",
      "2121 (15.00%) candidate strong lenses caught in filter 2\n"
     ]
    }
   ],
   "source": [
    "if survey_params['total_population']:\n",
    "    filters = util.unpickle_all(data_dir, 'filtered_sample_')\n",
    "\n",
    "    num_filter_1 = np.sum([d['num_filter_1'] for d in filters])\n",
    "    num_filter_2 = np.sum([d['num_filter_2'] for d in filters])\n",
    "\n",
    "    percent_filter_1 = num_filter_1 / len(total_df) * 100\n",
    "    percent_filter_2 = num_filter_2 / len(total_df) * 100\n",
    "\n",
    "    print(f'{num_filter_1} ({percent_filter_1:.2f}%) candidate strong lenses caught in filter 1')\n",
    "    print(f'{num_filter_2} ({percent_filter_2:.2f}%) candidate strong lenses caught in filter 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation time\n",
    "\n",
    "## Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00': '0:32:54',\n",
      " '01': '0:00:04',\n",
      " '02': '0:00:23',\n",
      " '03': '3:42:11',\n",
      " '04': '0:01:01',\n",
      " '05': '0:00:05'}\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(pipeline_dir, 'execution_times.json')\n",
    "\n",
    "import json\n",
    "with open(json_path, 'r') as f:\n",
    "    execution_times = json.load(f)\n",
    "\n",
    "pprint(execution_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pipeline execution time: 15398 seconds or 4:16:38 (4.28 hours)\n"
     ]
    }
   ],
   "source": [
    "total_time = 0\n",
    "\n",
    "for script_name, times in execution_times.items():\n",
    "    h, m, s = times.split(':')\n",
    "    if 'day' in h:\n",
    "        d, h = h.split('day, ')\n",
    "        h = int(d) * 24 + int(h)\n",
    "    time = (int(h) * 3600) + (int(m) * 60) + int(s)\n",
    "    total_time += time\n",
    "\n",
    "print(f'Total pipeline execution time: {total_time} seconds or {datetime.timedelta(seconds=total_time)} ({total_time / 3600:.2f} hours)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00': '12.82%',\n",
      " '01': '0.03%',\n",
      " '02': '0.15%',\n",
      " '03': '86.58%',\n",
      " '04': '0.40%',\n",
      " '05': '0.03%'}\n"
     ]
    }
   ],
   "source": [
    "percentage_dict = {}\n",
    "\n",
    "for script_name, times in execution_times.items():\n",
    "    h, m, s = times.split(':')\n",
    "    if 'day' in h:\n",
    "        d, h = h.split('day, ')\n",
    "        h = int(d) * 24 + int(h)\n",
    "    time = (int(h) * 3600) + (int(m) * 60) + int(s)\n",
    "    percentage = time / total_time * 100\n",
    "    percentage_dict[script_name] = f'{percentage:.2f}%'\n",
    "\n",
    "pprint(percentage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survey simulation time per square degree: 1096.67 seconds or 0:18:17 (18.28 minutes)\n"
     ]
    }
   ],
   "source": [
    "h, m, s = execution_times['00'].split(':')\n",
    "survey_sim_seconds = (int(h) * 3600) + (int(m) * 60) + int(s)\n",
    "\n",
    "print(f'Survey simulation time per square degree: {survey_sim_seconds / total_area:.2f} seconds or {datetime.timedelta(seconds=round(survey_sim_seconds / total_area))} ({survey_sim_seconds / total_area / 60:.2f} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image simulation time per image: 130.28 seconds\n"
     ]
    }
   ],
   "source": [
    "total_image_sim = 0\n",
    "image_sim_scripts = ['01', '02', '03', '04']\n",
    "\n",
    "for script_name in image_sim_scripts:\n",
    "    h, m, s = execution_times[script_name].split(':')\n",
    "    if 'day' in h:\n",
    "        d, h = h.split('day, ')\n",
    "        h = int(d) * 24 + int(h)\n",
    "    time = (int(h) * 3600) + (int(m) * 60) + int(s)\n",
    "    total_image_sim += time\n",
    "\n",
    "print(f'Image simulation time per image: {total_image_sim / len(detectable_df):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': '0.03%', '02': '0.17%', '03': '99.34%', '04': '0.45%'}\n"
     ]
    }
   ],
   "source": [
    "image_sim_percentage_dict = {}\n",
    "\n",
    "for script_name in image_sim_scripts:\n",
    "    h, m, s = execution_times[script_name].split(':')\n",
    "    if 'day' in h:\n",
    "        d, h = h.split('day, ')\n",
    "        h = int(d) * 24 + int(h)\n",
    "    time = (int(h) * 3600) + (int(m) * 60) + int(s)\n",
    "    percentage = time / total_image_sim * 100\n",
    "    image_sim_percentage_dict[script_name] = f'{percentage:.2f}%'\n",
    "\n",
    "pprint(image_sim_percentage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subhalo statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_list = util.unpickle_all(os.path.join(config.machine.dir_02, 'stats'), 'subhalo_stats_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_einstein_radii = [d['original_einstein_radius'] for d in stats_list]\n",
    "# adjusted_einstein_radii = [d['adjusted_einstein_radius'] for d in stats_list]\n",
    "# percent_change_einstein_radii = [d['percent_change_einstein_radius'] for d in stats_list]\n",
    "# effective_lensing_masses = [d['effective_lensing_mass'] for d in stats_list]\n",
    "# adjusted_lensing_masses = [d['adjusted_lensing_mass'] for d in stats_list]\n",
    "# percent_change_lensing_masses = [d['percent_change_lensing_mass'] for d in stats_list]\n",
    "# total_masses_subhalos_within_einstein_radius = [d['total_mass_subhalos_within_einstein_radius'] for d in stats_list]\n",
    "# total_subhalo_masses = [d['total_subhalo_mass'] for d in stats_list]\n",
    "# percent_subhalo_masses_within_einstein_radius = [d['percent_subhalo_mass_within_einstein_radius'] for d in stats_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "# ax[0].hist(percent_change_einstein_radii)\n",
    "# ax[0].set_xlabel('Percent Change in Einstein Radius')\n",
    "# ax[0].set_ylabel('Number of Lenses')\n",
    "# ax[1].hist(percent_change_lensing_masses)\n",
    "# ax[1].set_xlabel('Percent Change in Lensing Mass')\n",
    "# ax[1].set_ylabel('Number of Lenses')\n",
    "# ax[2].hist(percent_subhalo_masses_within_einstein_radius)\n",
    "# ax[2].set_xlabel('Percent of Subhalo Mass within Einstein Radius')\n",
    "# ax[2].set_ylabel('Number of Lenses')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = np.mean(percent_change_einstein_radii)\n",
    "# stdev = np.std(percent_change_einstein_radii)\n",
    "\n",
    "# print(f'Mean percent change in Einstein radius: {mean:.2f}')\n",
    "# print(f'Standard deviation of percent change in Einstein radius: {stdev:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mejiro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
